# Idiocratic Much?

After writing about [how companies get distracted by competitive dynamics](https://github.com/k1monfared/notes/blob/main/blog/20250913_foundation_or_competition.md), I got some great feedback that helped me work through some deeper thoughts.

## Cherry-Picking and What It Reveals

A friend pointed out that I was cherry-picking examples where focusing on fundamentals worked. They're absolutely right. Statistically speaking, shiny things win more often than fundamental solutions. That's exactly the problem I'm trying to highlight.

The fact that flashy, immediately impressive solutions usually win isn't evidence that markets work well - it's evidence of a systematic pattern. My examples of Google, Signal, and AWS are noteworthy precisely because they're exceptions to the rule. Most companies that ignored competitive pressure and focused on boring fundamentals actually failed. Yes, I'm oversimplifying - Google and AWS did respond to market forces, just by optimizing for solving problems well initially. But notice what happened after their success: Google moved from optimizing for search quality to optimizing for ad revenue. The pattern is clear - initial success through genuine value creation, followed by a shift toward extraction and engagement maximization.

This creates a selection pressure that pushes everyone toward optimization for perception rather than reality. It's like the [rat experiments](https://skeptics.stackexchange.com/questions/43198/will-rats-given-the-ability-to-artificially-stimulate-their-pleasure-centers-sta#43213) where rats stimulate their pleasure centers until they die - except now the pleasure center adjusts its voltage to keep us alive but never satisfied. We end up with an economy optimized for engagement and short-term metrics rather than genuine human flourishing.

## The Success Paradox

Apple and Meta and their ilk represent where our society is failing itself. Yes, they solved some real problems initially - iPhones enable video calls, navigation, access to information. I'm not denying the genuine utility. But that's not why they've "remained" successful by market measures. They're successful because they've perfected the art of exploiting human psychological vulnerabilities.

The benefits they provide could have been achieved without the manipulative aspects. We could have had communication tools without infinite scroll, without algorithmic rage amplification, without the surveillance. After solving a problem or two, they became completely exploitative. They're the rat side of us humans that keeps pressing the electric shock button, except they're not helpless buttons - they adjust the amount of shock so we don't die but keep pressing.

When people point to "revealed preferences - people love their iPhones!" that's exactly like saying the rats "loved" pressing the pleasure button. The behavior doesn't indicate genuine well-being.

## The Measurement Trap

This gets at something fundamental about how we've structured our world. Just like institutions use GPA as a proxy for intelligence and then forget it's just a proxy, we use purchasing decisions and engagement metrics as proxies for value creation and forget they're just proxies. We end up optimizing for the measurement instead of the thing we actually care about.

The problem isn't just that these proxies are imperfect - it's that optimizing for the proxy actively distorts the underlying reality. When companies optimize for engagement metrics, they don't just fail to create genuine value - they actively destroy our capacity for sustained attention and deep satisfaction. When we optimize for looking successful on social media, we don't just fail to build authentic relationships - we actively undermine our ability to connect genuinely with others.

Look at what's happened to educational content. If you want to create educational videos on TikTok that actually get seen, you have to make them narrated by robotic voices over footage of someone playing a first-person shooting game, moving from one space to another space. This isn't because it's better for learning - it's because that's how attention spans have been trained. That's what it takes for a teenager to focus these days. We've degraded educational content to match degraded attention spans, which further degrades those attention spans in an endless spiral downward.

We've created systems where the easy thing to measure (clicks, time spent, purchases made) becomes what gets optimized for, even when it's fundamentally at odds with what we actually want (meaningful experiences, genuine connections, tools that serve our real needs).

## We All Participate

Individuals absolutely do optimize strategically, just like companies do. Look at someone's dating profile or Instagram page. These are carefully curated performances designed to give signals they think will win. Someone posting gym selfies even though they hate working out is doing exactly what Yahoo did with their feature-packed portal: optimizing for what they think the market wants rather than authentically representing what they actually offer.

A friend argued this is just normal human behavior - people have always presented idealized versions in courtship. That's true. We've always chosen shiny things over everything else - there's an evolutionary need for it. Bigger muscles meant protection, signs of fertility meant healthy offspring. But we've reached a point where these signals are completely manufactured and no longer serve their original purpose. Those muscles are steroids and photoshop. We're still hardwired to respond to these signals, even though they're now fake and don't indicate anything useful.

The scale has fundamentally changed. We've gone from "wear your best clothes to the dance" to "spend 20 hours a week maintaining a fake persona that algorithms reward because it generates engagement." Same evolutionary impulse, completely different scale and purpose. We're not just choosing badly, we're being systematically exploited through evolutionary vulnerabilities that worked fine when signals were honest but catastrophically backfire when signals can be manufactured at scale - and we're exploiting those vulnerabilities ourselves too, both in others and in ourselves.

The tragedy is that this creates competitive pressure on everyone else. Once enough people are posting adventure photos, everyone feels pressure to do the same or risk looking boring. Just like once competitors start adding AI features, every company feels they need AI features.

## Snake Oil vs Fishermen

A friend mentioned they see some of my examples as generalist vs specialist strategies. That's one way to look at it, but I see it more like snake oil sellers vs fishermen. The distinction isn't about breadth vs depth - it's about intent.

In healthcare, there's real cooperation between generalists and specialists. My general practitioner constantly refers me to specialists, and vice versa. There's an ethical framework where the job is to get you the best care, even if that means losing you as a patient to someone else.

But Apple wants to be your phone, your computer, your watch, your music, your movies, your payments, your identity verification, your fitness tracker, your car interface. They don't just want to be the hospital, they want to be the entire city. The difference is between "I want to be the best at solving this specific problem for you" versus "I want you to never need to go anywhere else." One is about serving you, the other is about capturing you.

## The Feedback Loop of Degradation

Here's where economists and I really diverge. They think markets self-correct in the long term. I think the market doesn't "correct" itself toward better human outcomes - it responds in whatever direction seems more profitable in the short term. It's essentially running a greedy algorithm that looks maybe 2-3 steps ahead instead of finding globally optimal solutions.

But here's the deeper problem: we're actively changing what the market responds to. We're training ourselves to only seek immediate pleasure and convenience, regardless of long-term consequences. We're creating an addicted economy where the market doesn't respond to "good" products anymore - it only responds to whatever triggers our dopamine systems most effectively.

This creates a vicious feedback loop. Our degraded preferences lead companies to build more addictive, manipulative products. These products further degrade our ability to recognize genuine value. Which leads us to reward even more sophisticated manipulation techniques. We're not just getting worse products - we're actively getting worse at wanting better products.

Each time we choose immediate gratification, we get a little worse at resisting the next manipulation. It's like we're halfway to [Idiocracy](https://www.imdb.com/title/tt0387808/), or even past it. Brawndo has electrolytes because that's what plants crave, and everyone's forgotten what plants actually need. We're training the market to give us digital Brawndo while forgetting what human flourishing actually requires.

## Are We Living in the Debris?

Here's something darker to consider: what if this has been happening for centuries? Think about Justin Bieber. It's entirely possible that 200 years from now, when people listen to our era's music, he's one of the survivors. If from thousands of artists, he's one that remains, what does that mean? That we had really low quality music selected as representative of our era. Pink Floyd in the dumps, Philip Glass down the sink, Wynton Marsalis down the toilet.

Now backtrack the same way. What if Beethoven and Bach and Mozart were the Justin Biebers of their times? What if we've lost all traces of the real geniuses of those eras simply because the market demanded more accessible, broadly appealing music?

We might be living in the accumulated debris of centuries of market selection, where each generation's trash becomes the next generation's canonical culture. The "masterpieces" we celebrate might just be what survived because it had mass appeal, not because it was genuinely the best. Every market success - touch interfaces, streaming algorithms, social media - just accelerates this process, making it harder to choose quality even if you want to, because the market has already eliminated those options.

## A Counterexample Worth Examining

A friend pointed out that Nordic countries actually implemented intense economic liberalization policies. They weren't anti-market holdouts like I might have suggested. But this actually makes the example better.

The Nordic countries were selective about which competitive pressures to respond to and which to resist. They liberalized markets where competition seemed to serve genuine efficiency, but they didn't abandon their foundational commitments to education, healthcare, and social safety nets just because other countries were cutting those things.

They made conscious choices about which signals from the competitive environment to follow and which to ignore, based on deeper principles about what they were trying to achieve for their people. This required citizens who actively demanded something better than the shiny, immediate option.

But notice how fragile this is. Even the EU is currently caving to AI company demands and citizen pressure for access to these tools. The "right governance" doesn't come to power without citizens demanding it, and maintaining it requires constant effort against our evolutionary biases toward shiny things.

## The Erosion of Choice Itself

Here's what really concerns me: these shiny things aren't just winning - they're eliminating the possibility of alternatives. 

Someone might say people still have free will, that preferences can be manipulated a little but fundamentally people are choosing. I think we're witnessing something more disturbing: the choice architecture has been so thoroughly corrupted that what looks like "free will" is actually addiction running on autopilot.

Take the digitizer pen example. Touch interfaces being accessible is great - but why did precision input devices have to nearly disappear from the market? Even if you wanted to choose better digitizer technology, those choices no longer exist. Your free will to choose is constrained by what the market makes available.

Try buying a "dumb" phone with good build quality. Try finding social media that doesn't optimize for engagement. Try getting healthcare apps that don't gamify your data. These choices don't exist anymore. We're not just choosing poorly - we're living in a choice desert where only junk food options remain.

This is how free will gets eroded: not through direct force, but through the systematic elimination of alternatives until the addictive choice is the only choice left.

## Working Through This

I should acknowledge - a friend pointed out that my evidence is mostly anecdotal. They wanted systematic data on preference degradation, not just observations about disappearing product categories. That's fair. These are examples that concern me, not proof of a universal pattern. Consider this a hypothesis worth investigating: are we systematically selecting for products that exploit psychological vulnerabilities at the expense of those that serve long-term interests?

But here's the thing - the erosion of choice isn't just about preferences, it's about what options exist at all. That's observable. You can't prove people's desires have fundamentally changed, but you can point to a market structure that increasingly offers only one type of solution: the addictive, extractive kind.

## What This Means for Us

I'm not blaming "the market" as some external force. I'm raising an alarm about our collective behavior. We are the market. When you buy a Tesla, how much of that decision is based on actual needs versus advertising versus social proof? Most people can't answer that - they don't even realize the question matters.

Markets will always respond to whatever signals we send. If we send signals based on ads and what our friends bought (who bought based on what their friends bought), then markets will optimize for creating compelling ads and social proof. If we sent signals based on actual needs and careful evaluation, markets would optimize for genuine value.

To be honest, I have no clue what the solution is. But we as end users have a responsibility to give better signals to an economy that uses whatever signals it can measure. It's unfortunately a new burden on our shoulders - being conscious about why we make choices when we do make them.

I don't buy Apple products. I try not to buy big company products (though it's extremely difficult). But this isn't "convenient" for most people, who give up everything including their privacy for a bit more convenience. The energy required to constantly swim against these systems is enormous.

Most movements to "raise awareness" about these issues become temporary hypes that eventually get co-opted or fade away. Even "digital wellness" has been turned into apps and products to sell back to us. We're now entrusting the very companies that got us addicted to help us manage our addiction, but not cure it, just manage it enough so we can keep our jobs and keep paying for the substance.

The real problem isn't that people are making bad choices, but that we've structured the environment so that the easy, default choices serve corporate interests rather than human flourishing. The shiny solutions don't just win - they actively kill off alternatives, even when those alternatives are technically superior.

I don't think there's an easy answer to this, or any answer at all. But recognizing that we're actively participating in a system that optimizes for the wrong things - and that this optimization actively crowds out better solutions - might be the first step toward understanding why we end up with a world full of addictive products instead of genuinely useful tools.